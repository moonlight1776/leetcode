决策树可以看成为一个 if-then 规则的集合，即由决策树的根节点到叶节点的每一条路径构建一条规则，路径上内部节点的特征对应着规则的条件，而叶节点的类对应于规则的结论。因此决策树就可以看作由条件 if（内部节点）和满足条件下对应的规则 then（边）组成。
决策树的工作方式是以一种贪婪（greedy）的方式迭代式地将数据分成不同的子集。其中回归树（regression tree）的目的是最小化所有子集中的 MSE（均方误差）或 MAE（平均绝对误差）；而分类树（classification tree）则是对数据进行分割，以使得所得到的子集的熵或基尼不纯度（Gini impurity）最小。
通过将许多决策树组成森林并为一个变量取所有树的平均贡献，这个确定特征的贡献的过程可以自然地扩展成随机森林。
回归树的特征分布源自环的平均值以及其在后续分割中的变化方式。我们可以通过检查每个子集中某个特定类别的观察的比例，从而将其扩展成二项分类或多项分类。一个特征的贡献就是该特征所导致的总的比例变化。

![案例：预测鲍鱼是雌性、雄性还是幼体](https://upload-images.jianshu.io/upload_images/5220317-ab597023391cfbd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
```
决策树常见参数和概念

如果我们希望以数学的方式理解决策树，我们首先需要了解决策树和树型学习算法的一般概念。理解以下的术语同样能帮助我们调整模型。

根结点：表示所有数据样本并可以进一步划分为两个或多个子结点的父结点。
分裂（Splitting）：将一个结点划分为两个或多个子结点的过程。
决策结点：当一个子结点可进一步分裂为多个子结点，那么该结点就称之为决策结点。
叶/终止结点：不会往下进一步分裂的结点，在分类树中代表类别。
分枝/子树：整棵决策树的一部分。
父结点和子结点：如果一个结点往下分裂，该结点称之为父结点而父结点所分裂出来的结点称之为子结点。
结点分裂的最小样本数：在结点分裂中所要求的最小样本数量（或观察值数量）。这种方法通常可以用来防止过拟合，较大的最小样本数可以防止模型对特定的样本学习过于具体的关系，该超参数应该需要使用验证集来调整。
叶结点最小样本数：叶结点所要求的最小样本数。和结点分裂的最小样本数一样，该超参数同样也可以用来控制过拟合。对于不平衡类别问题来说，我们应该取较小的值，因为属于较少类别的样本可能数量上非常少。
树的最大深度（垂直深度）：该超参数同样可以用来控制过拟合问题，较小的深度可以防止模型对特定的样本学习过于具体的关系，该超参数同样需要在验证集中调整。
叶结点的最大数量：叶结点的最大个数可以替代数的最大深度这一设定。因为生成一棵深度为 n 的二叉树，它所能产生的最大叶结点个数为 2^n。
分裂所需要考虑的最大特征数：即当我们搜索更好分离方案时所需要考虑的特征数量，我们常用的方法是取可用特征总数的平方根为最大特征数。
```
##**决策树的局限性**

决策树有很多优点，比如：

易于理解、易于解释
可视化
无需大量数据准备。不过要注意，sklearn.tree 模块不支持缺失值。
使用决策树（预测数据）的成本是训练决策时所用数据的对数量级。

但这些模型往往不直接使用，决策树一些常见的缺陷是：

构建的树过于复杂，无法很好地在数据上实现泛化。
数据的微小变动可能导致生成的树完全不同，因此决策树不够稳定。
决策树学习算法在实践中通常基于启发式算法，如贪婪算法，在每一个结点作出局部最优决策。此类算法无法确保返回全局最优决策树。
如果某些类别占据主导地位，则决策树学习器构建的决策树会有偏差。因此推荐做法是在数据集与决策树拟合之前先使数据集保持均衡。
某些类别的函数很难使用决策树模型来建模，如 XOR、奇偶校验函数（parity）和数据选择器函数（multiplexer）。

####**剪枝**

由于决策树容易对数据产生过拟合，因此分支更少（即减少区域 R_1, … ,R_J）的小树虽然偏差略微高一点，但其产生的方差更低，可解释性更强。处理上述问题的一种方法是构建一棵树，每个分支超过某个（高）阈值造成叶结点误差率 Qm 下降，则结束构建。但是，由于分裂算法的贪婪本质，它其实很短视。决策树早期看似无用的一次分裂有可能会导致之后一次优秀的分裂，并使得 Qm 大幅下降。

#### **袋装（Bootstrap Aggregating——Bagging）**

在统计学中，Bootstrap 是依靠替换随机采样的任意试验或度量。我们从上文可以看见，决策树会受到高方差的困扰。这意味着如果我们把训练数据随机分成两部分，并且给二者都安置一个决策树，我们得到的结果可能就会相当不同。Bootstrap 聚集，或者叫做袋装，是减少统计学习方法的方差的通用过程。

给定一组 n 个独立的样本观测值 Z_1，Z_2，...，Z_n，每一个值的方差均为 *σ^*2，样本观测值的均值方差为 *σ^*2/*n*。换句话说，对一组观测值取平均会减小方差。因此一种减小方差的自然方式，也就是增加统计学习方法预测精度的方式，就是从总体中取出很多训练集，使用每一个训练集创建一个分离的预测模型，并且对预测结果求取平均值。

这里有一个问题，即我们不能获取多个训练数据集。相反，我们可以通过从（单一）训练数据集提取重复样本进行自助法（bootstrap）操作。在这种方法中，我们生成了 B 个不同的自助训练数据集。我们随后在第 b 个自助训练数据集得到了一个预测结果![image](http://upload-images.jianshu.io/upload_images/5220317-d06919f48fd140a0?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

，从而获得一个聚集预测（aggregate prediction）。

![image](http://upload-images.jianshu.io/upload_images/5220317-9372a0792cff47ac?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这就叫做袋装（bagging）。注意，聚集（aggregating）在回归和分类问题中可能有不同的均值。当平均预测值在回归问题中的效果很好时，我们将会需要使用多数票决（majority vote）：由于分类问题中的聚集机制，整体预测就是在 B 个预测值中最常出现的那个主要类别。

##**随机森林模型**

虽然袋装技术（Bagging）通过降低方差而提高了一般决策树的预测性能，但它还遇到了其他缺点：Bagging 要求我们在自助样本上生成整棵树，这就增加了 B 倍计算复杂度。此外，因为基于 Bagging 的树是相关联的，预测精度会根据 B 而饱和。

随机森林通过随机扰动而令所有的树去相关，因此随机森林要比 Bagging 性能更好。随机森林不像 Bagging，在构建每一棵树时，每一个结点分割前都是采用随机样本预测器。因为在核心思想上，随机森林还是和 Bagging 树一样，因此其在方差上有所减少。此外，随机森林可以考虑使用大量预测器，不仅因为这种方法减少了偏差，同时局部特征预测器在树型结构中充当重要的决策。

随机森林可以使用巨量的预测器，甚至预测器的数量比观察样本的数量还多。采用随机森林方法最显著的优势是它能获得更多的信息以减少拟合数值和估计分割的偏差。

通常我们会有一些预测器能主导决策树的拟合过程，因为它们的平均性能始终要比其他一些竞争预测器更好。因此，其它许多对局部数据特征有用的预测器并不会选定作为分割变量。随着随机森林计算了足够多的决策树模型，每一个预测器都至少有几次机会能成为定义分割的预测器。大多数情况下，我们不仅仅只有主导预测器，特征预测器也有机会定义数据集的分割。
#####**随机森林有三个主要的超参数调整：**
结点规模：随机森林不像决策树，每一棵树叶结点所包含的观察样本数量可能十分少。该超参数的目标是生成树的时候尽可能保持小偏差。
树的数量：在实践中选择数百棵树一般是比较好的选择。
预测器采样的数量：一般来说，如果我们一共有 D 个预测器，那么我们可以在回归任务中使用 D/3 个预测器数作为采样数，在分类任务中使用 D^(1/2) 个预测器作为抽样


